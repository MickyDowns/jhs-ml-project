---
title: "jh7-pml-wk2"
output: html_document
---

# working w/ carat
## split the data
```{r}
library(caret)
library(kernlab); data(spam)
inTrain=createDataPartition(y=spam$type,p=0.75, list=FALSE)
# split on type, split 75/25
# so, this removed the sample step and probably a cbind.
training=spam[inTrain,]
testing=spam[-inTrain,]
dim(training)

```

## fit a model
```{r}
set.seed(32343)
modelFit=train(type~.,data=training,method="glm")
modelFit
modelFit$finalModel
```

## predict
```{r}
predictions=predict(modelFit,newdata=testing)
table(predictions)
confusionMatrix(predictions,testing$type)
```

## data splitting
```{r}
# see previous test/train split.
# for k-fold
set.seed(32323)
folds=createFolds(y=spam$type,k=10,list=TRUE,returnTrain=TRUE)
# list = TRUE returns each set of indices for a fold as a list
# returnTrain returns the training set or not.
# for returnTrain=FALSE, it returns just the test set samples
sapply(folds,length)
# so, it split data set

folds[[1]][1:10]
# look at elements that appeared in 1st fold

folds=createFolds(y=spam$type,k=10,list=TRUE,returnTrain=FALSE)
sapply(folds,length) # now it's just the test records. 
folds[[1]][1:10]

```

## resampling
```{r}
set.seed(32323)
folds=createResample(y=spam$type,times=10,list=TRUE)
# dim(spam) is 4601, so this is bootstrapping (replace=TRUE)
# tell it if you want a list, matrix or vector
sapply(folds,length)
folds[[1]][1:10]
# getting dups as you've got replace=TRUE
```

## time slices
```{r}
set.seed(32323)
# for anlayizing data for forecasting
# time vector
tme=1:1000
folds=createTimeSlices(y=tme,initialWindow=20,horizon=10)
# create time slices that have a window of 20 samples each. 
# predict the next 10 samples out
names(folds)
folds$train[[1]]
folds$test[[1]]
# so, this is grabbing 20 train, 10 test, 20 train, 10 test, etc.
```

## training
```{r}
# load data as before. 
# basic approach is as follows
modelFit=train(type~.data=training,method="glm")

# going further
args(train.default)

#function(x,y,method="rf",pre_rocess=NULL,# number of options
#         ...,weights=NULL,# up or down-weight certain observations. useful if you have unbalanced training set. 
#         metric=ifelse(is.factor(y),"Accuracy","RMSE"),
         # default metric for classification is accuracy. for regression is RMSE. 
#         maximize=ifelse(metric=="RMSE",FALSE,TRUE),
#         trControl=trainControl(),# set other parms
#         tuneGrid=NULL,
#         tuneLength=3)
#NULL

args(train.control)

```

## plotting data
```{r}
library(ISLR)
data(Wage); summary(Wage)

inTrain=createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training=Wage[inTrain,]; testing=Wage[-inTrain,]

# looking at relatinship w/ y var
featurePlot(x=training[,c("age","education","jobclass")],
            y=training$wage,plot="pairs")

# encoding y var, meh
featurePlot(x=training[,c("age","education","jobclass")],
            y=ifelse(training$wage>200,1,0),
            plot="pairs",col=c("red","blue"))

qplot(age,wage,data=training)
qplot(age,wage,colour=jobclass,data=training)
qplot(age,wage,colour=maritl,data=training)
qplot(age,wage,colour=education,data=training)
qplot(age,wage,colour=race,data=training)

qq=qplot(age,wage,colour=education,data=training)
qq+geom_smooth(method='lm',formula=y~x)
# very slick, easy way to plot lines for each subgroup

# cutting and boxplots
library(Hmisc)
cutWage=cut2(training$wage,g=3)
table(cutWage) # cuts data into factors based on quantile groups
p1=qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot")); p1

qplot(maritl,age,data=training,fill=maritl,geom=c("boxplot"))

# overlays
library(gridExtra)
p2=qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)

# tables
t1=table(cutWage,training$jobclass); t1 # shows higher wages for info
prop.table(t1,1) # v. slick. creates proportions of each row. 
prop.table(t1,2) # or column.

# density plots for continuous predictors
qplot(wage,colour=education,data=training,geom="density") # always a good (if not best) one.

```

## preprocessing
often more useful in parametric than non parametric use cases
```{r}
data(spam)
inTrain=createDataPartition(y=spam$type,p=0.75,list=FALSE)
training=spam[inTrain,]
testing=spam[-inTrain,]
hist(training$capitalAve,main="",xlab="ave cap run length")
# highly skewed
mean(training$capitalAve); sd(training$capitalAve)
# so, standardize
trainCapAve=training$capitalAve
trainCapAveS=(trainCapAve-mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS);sd(trainCapAveS)

# my own attempt
mean(scale(trainCapAve));sd(scale(trainCapAve))
hist(log(training$capitalAve))
hist(scale(log(training$capitalAve))) # scaling 1st intro's nan's.

# standardizing the test set: we can only apply predictors to test set that we've estimated in training. so, when we apply standardization to test, WE HAVE TO USE MEAN AND SD FROM TRAINING SET.
testCapAve=testing$capitalAve
testCapAveS=(testCapAve-mean(trainCapAve))/sd(trainCapAve)
# aha, so this is why they didn't just use scale. 
mean(testCapAveS);sd(testCapAveS)
# result is that mean is no longer exactly 0 and sd no longer 1.

# using pre-process function to creat an object (function) that does scaling.
preObj=preProcess(training[,-58],method=c("center","scale"))
trainCapAveS=predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS);sd(trainCapAveS)

## now you can use same object to pre-process data from test set
testCapAveS=predict(preObj,testing[-58])$capitalAve
mean(testCapAveS);sd(testCapAveS)

## you can also pass those pre-process object/functions to the caret train function as an argument
set.seed(32343)
modelFit=train(type~.,data=training,
               preProcess=c("center","scale"),method="glm")
modelFit

# other kinds of transformations
## box-cox transforms
preObj=preProcess(training[,-58],method=c("BoxCox"))
trainCapAveS=predict(preObj,training[,-58])$capitalAve
par(mfrow=c(2,2));hist(trainCapAveS);qqnorm(trainCapAveS)
# box cox takes continuous and makes them look normal by estimating parms using estimated likelihood. note from plot, it has trouble with 0's and values that are repeated. 

# here are the methods available:  BoxCox, YeoJohnson, expoTrans, center, scale, range, knnImpute, bagImpute, medianImpute, pca, ica, spatialSign

preObj=preProcess(training[,-58],method=c("YeoJohnson"))
trainCapAveS=predict(preObj,training[,-58])$capitalAve
hist(trainCapAveS);qqnorm(trainCapAveS)

# imputing missing data
library(RANN)
set.seed(13343)
## make some values NA
training$capAve=training$capitalAve
selectNA=rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA]=NA
### so, capAve is like capitalAve, but w/ missing values.

## impute and standardize
preObj=preProcess(training[,-58],method="knnImpute")
capAve=predict(preObj,training[,-58])$capAve

## standardize true values
capAveTruth=training$capitalAve
capAveTruth=(capAveTruth-mean(capAveTruth))/sd(capAveTruth)

## check results
quantile(capAve-capAveTruth) 
### for all values, w/in a couple percent
quantile((capAve-capAveTruth)[selectNA])
### looking just at values that were missing, again w/in couple pct
quantile((capAve-capAveTruth)[!selectNA])
### double check to ensure baselines are similar
### note the slick way of indexing both variables outside the parens.

```

# creating covariates
```{r}
data(Wage)
inTrain=createDataPartition(y=Wage$wage,p=0.7,list=FALSE) # creates indices for two data sets
training=Wage[inTrain,];testing=Wage[-inTrain,]


# this is making a binary dummy var from a qualitative (text) variable. ## it's doing it by predicting jobclass based on wage.
head(training$jobclass)
table(training$jobclass)
dummies=dummyVars(wage~jobclass,data=training)
head(predict(dummies,newdata=training))
## so, we're netting an inidcator for industrial vs. 

# removing covariates w/ a lot of zeros as they tend not to be good predictors as they have little if any variability. 
nsv=nearZeroVar(training, saveMetrics=TRUE)
nsv
## you can see it's flagged sex and region as having low levels of unique values.
## MVD YOU SHOULD USE THIS TO SELECT OUT COLLINEAR FEATURES IN VALUE

# spline basis to fit curvy lines
library(splines)
bsBasis=bs(training$age,df=3)
head(bsBasis)
## gives you the degree 1-3 poly results
lm1=lm(wage~bsBasis,data=training)
par(mfrow=c(1,1))
plot(training$age,training$wage,pch=19,cex=0.5)
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)

## on test set
predict(bsBasis,age=testing$age) # you need to use same function to create variables on test set. 
```

## preprocessing w/ pca
```{r}
# using same spam data set
data(spam)
inTrain=createDataPartition(y=spam$type,p=0.75,list=FALSE)
training=spam[inTrain,]
testing=spam[-inTrain,]

M=abs(cor(training[,-58])) # find predictors w/ high correlation
diag(M)=0 # set cor for vars w/ themselves to 0
which(M>0.8,arr.ind=T) # find the highly correlated vars

names(spam)[c(34,32)] # looks like phone numbers
plot(spam[,34],spam[,32]) # very highly correlated
## including both won't be useful. can we make them one? say a weighted one? 

# so, interesting that they're doing pc on just two dimensions. 
smallSpam=spam[,c(34,32)]
prComp=prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])
## most variability along pc1 dimension.

prComp$rotation
# amount of adjustment, 1st principle comp is adding vars, second is taking difference.

# pca on the spam data
typeColor=((spam$type=="spam")*1+1) # color-code records based on spam
prComp=prcomp(log10(spam[,-58]+1)) # run pcomp on log of entire X
## ofen have to do the log10 + 1 transformation to continuous for pca to make it look more gaussian.
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
## WHOA, THIS IS QUICK WAY TO SEE IF THERE'S ANY THERE, THERE

# doing it in caret
preProc=preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC=predict(preProc,log10(spam[,-58]+1)) # this step is using pca to predict principal components of the data set.
plot(spamPC[,1],spamPC[,2],col=typeColor)

# pre-processing w/ pca
preProc=preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC=predict(preProc,log10(training[,-58]+1))
modelFit=train(training$type~.,method="glm",data=trainPC) 
## so this is fitting a regression model on the principal components.
## automatically fit logistic model b/c type=factor var. 
## need to apply training principal components to test set. no re-genning prin comp on test set
testPC=predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
## note: minor pickup from using 3 principal components. 

## alternative approach using preProcess as a sub-function of train()
modelFit=train(training$type~.,method="glm",preProcess="pca",data=training)
confusionMatrix(testing$type,predict(modelFit,testing))
```

## predicting w/ regression
```{r}
data(faithful); set.seed(333)
inTrain=createDataPartition(y=faithful$waiting,p=0.5,list=FALSE)
trainFaith=faithful[inTrain,];testFaith=faithful[-inTrain,]
head(trainFaith)

plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="waiting",ylab="duration")
# let's plot eruption duration as a function of wait. 
## that'll be ED_i=b_0+b_1WT_i+e_i
lm1=lm(eruptions~waiting,data=trainFaith); summary(lm1)

plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue")
lines(trainFaith$waiting,lm1$fitted,lwd=3) ## really? all you have to do is grab $fitted?

# predict how long it will erupt if you wait 80 minutes:
## that'll be estWD=estb_0+estb_1WT
coef(lm1)[1]+coef(lm1)[2]*80

newdata=data.frame(waiting=80)
predict(lm1,newdata)
points(80,predict(lm1,newdata),col="red",pch=19,cex=3)

## let's predict on test set
par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",main="train")
lines(trainFaith$waiting,predict(lm1),lwd=3)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",main="test")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)

# calc RMSE train and test
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))

# prediction intervals
pred1=predict(lm1,newdata=testFaith,interval="prediction")
## so, this tells predict to return the predicted values and the prediction interval - you could have also set se.fit=TRUE
ord=order(testFaith$waiting)
par(mfrow=c(1,1))
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",col=c(1,2,2),lty=c(1,1,1),lwd=1)

## w/ caret
modFit=train(eruptions~waiting,data=trainFaith,method="lm")
summary(modFit$finalModel)

```

## predicting w/ regression multiple covariates
```{r}
data(Wage)
Wage=subset(Wage,select=-c(logwage)) 
## selecting out var we're going to try to predict. 
summary(Wage)

inTrain=createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training=Wage[inTrain,];testing=Wage[-inTrain,]
dim(training);dim(testing)

# look at data
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
## outliers could be helpful

qplot(age,wage,data=training)
qplot(age,wage,colour=jobclass,data=training)
## looks like info group will help us predict
qplot(age,wage,colour=education,data=training)
## so will education
qplot(age,wage,colour=maritl,data=training)
## so will marital status

# so, let's fit a linear model
## will look like ED_i=b_0+b_1age+b_2I(jobclass_i="Information")+sum_k=1^4(gammaI(education=levelk))
modFit=train(wage~age+jobclass+education,method="lm",data=training)
# jobclass and education are factor vars. so, their auto converted to dummy vars. hence 10 vars below.
finMod=modFit$finalModel
print(modFit)

## let's plot
plot(finMod,1,pch=19,cex=0.5)
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
qplot(finMod$fitted,finMod$residuals,colour=maritl,data=training)
qplot(finMod$fitted,finMod$residuals,colour=education,data=training)
qplot(fitYr,redYr)
qplot(finMod$fitted[training$year %in% c(2003,2009)],
      finMod$residuals[training$year %in% c(2003,2009)],
      colour=year,
      data=training[training$year %in% c(2003,2009),])

# plot by index
plot(finMod$residuals,pch=19)
## indicates trend w/respect to row numbers. suggests there's a variable missing from your model. variance shouldn't have anything to do w/ your vars. so, this suggests there's hidden variation in terms of time, age or some other variable that is linked to the index. 

pred=predict(modFit,testing)
qplot(wage,pred,colour=year,data=testing)
## hmm... ideally you'd have a 45 degree line. not so. so, maybe it's a year thing. 
## can't go back to update training set. this is a post mortem

modFitAll=train(wage~.,data=training,method="lm")
pred=predict(modFitAll,testing)
qplot(wage,pred,data=testing)
```

## week 2 quiz
```{r}
# question 1
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)

# question 2
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain =createDataPartition(mixtures$CompressiveStrength,p=3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]

for(i in 1:dim(training)[2]){
     cut.i=cut2(training[,i],g=3)
     plot(training$CompressiveStrength,
          col=cut.i,
          main=names(training)[i])
}
     
# question 3
hist(training$Superplasticizer)
hist(log(training$Superplasticizer))
hist(log(training$Superplasticizer+1))

# question 4
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]

names(training) # cols 58:69
typeColor=((diagnosis=="Impaired")*1+1)
# doing it in caret
preProc=preProcess(adData[,58:69],method="pca",thresh=0.9)
adPC=predict(preProc,adData[,58:69]) # this step is using pca to predict principal components of the data set.
plot(adPC[,1],adPC[,2],col=typeColor)

preProc$numComp

# question 5

# base model
modelFit=train(diagnosis[inTrain]~.,method="glm",data=training[,58:69]) 
confusionMatrix(diagnosis[-inTrain],predict(modelFit,testing[,58:69])

# pca model
preProc=preProcess(training[,58:69],method="pca",thresh=0.8)
trainPC=predict(preProc,training[,58:69]+1)
modelFit=train(diagnosis[inTrain]~.,method="glm",data=trainPC) 
testPC=predict(preProc,testing[,58:69]+1)
confusionMatrix(diagnosis[-inTrain],predict(modelFit,testPC))

```
