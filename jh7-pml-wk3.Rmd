---
title: "jh7-pml-wk2"
output: html_document
---

# trees
## node purity
```{r}
library(caret)
data(iris); library(ggplot2)
table(iris$Species)

inTrain=createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training=iris[inTrain,]
testing=iris[-inTrain,]
dim(training); dim(testing)

qplot(Petal.Width,Sepal.Width,colour=Species,data=training)

library(caret)
modFit=train(Species~.,method="rpart",data=training)
print(modFit$finalModel)

plot(modFit$finalModel,uniform=FALSE,main="classification")
text(modFit$finalModel, use.n=TRUE,all=TRUE,cex=0.8)

#install.packages("rattle",dep=TRUE)
library(rattle)
fancyRpartPlot(modFit$finalModel)

predict(modFit,newdata=testing)
```

## bagging
```{r}
install.packages("ElemStatLearn")
library("ElemStatLearn")
data(ozone)
ozone=ozone[order(ozone$ozone),]
head(ozone)

# bagged loess
ll=matrix(NA,nrow=10,ncol=155) # matrix of 10x155
for(i in 1:10){  # loop over 10 samples
     ss=sample(1:dim(ozone)[1],replace=T)  # sample w/ replace
     
     ozone0=ozone[ss,];ozone0=ozone0[order(ozone0$ozone),] 
     # data set for that element of the loop. reorder by ozone variable
     
     loess0=loess(temperature~ozone,data=ozone0,span=0.2) 
     # fit loess or spline, smoothed curve relating temp to ozone.
     
     ll[i,]=predict(loess0,newdata=data.frame(ozone=1:155)) 
     # predict the outcome for new data set for new test set. 
}

plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
# resample, fit smooth curve, average results. 

# your own bagging function.
predictors=data.frame(ozone=ozone$ozone)
temperature=ozone$temperature
treebag=bag(predictors,temperature,B=10,
            bagControl=bagControl(fit=ctreeBag$fit,
                                  predict=ctreeBag$pred,
                                  aggregate=ctreeBag$aggregate))

# parts of bagging
ctreeBag$fit

function(x,y,...){
     # takes dataframe and outcome
     library(party)
     data=as.data.frame(x)
     data$y=y
     ctree(y~.,data=data) # returns model fit
}

ctreeBag$pred

function(object,x){ # takes model fit, new data set
     obslevels=levels(object@data@get("response")[,1])
     if(!is.null(obsLevels)){
          rawProbs=treeresponse(object,x) 
          # calcs response from tree obj using new data. calculates probability matrix or response variable
          probMatrix=matrix(unlist(rawProbs),ncol=length(obsLevels),
                            byrow=TRUE)
          out=data.frame(probMatrix)
          colnames(out)=obsLevels
          rownames(out)=NULL
     }
     else out=unlist(treeresponse(object,x))
     out
}

ctreeBag$aggregate

function(x,type="class"){ # takes values and averges them together. 
     if(is.matrix(x[[1]])|is.data.frame(x[[1]])){
          pooled=x[[1]]&NA
          classes=colnames(pooled)
          for(i in 1:ncol(pooled)){
               tmp=lapply(x,function(y,col),y[,col],col=i) # getting prediction from each model fit. 
               tmp=do.call("rbind",tmp) # binds them into a single matrix each row being a model prediction
               pooled[,i]=apply(tmp,2,median) # takes median prediction across bootstrap samples.
          }
          ...
     }
}
```

## random forest
```{r}
data(iris);library(ggplot2)
inTrain=createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training=iris[inTrain,]
testing=iris[-inTrain,]

modFit=train(Species~.,data=training,method="rf",prox=TRUE)
modFit

pred=predict(modFit,testing)
confusionMatrix(pred,testing$Species)

# let's look at a specific tree
getTree(modFit$finalModel,k=2)

# use centers of the predicted values
irisP=classCenter(training[,c(3,4)],training$Species,modFit$finalModel$prox) # send it model fit on training data using prox
irisP=as.data.frame(irisP); irisP$Species=rownames(irisP) # that give class centers. so, we've created centers and species data sets.
p=qplot(Petal.Width,Petal.Length,col=Species,data=training) # plot petal length by petal width 
p+geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP) # add points for petal width and length. color based on species. observation centers are x'd. 

# let's look at the two we missed
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="test preds")
```

## boosting
```{r}
library(ISLR);data(Wage)
Wage=subset(Wage,select=-c(logwage))
inTrain=createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training=Wage[inTrain,];testing=Wage[-inTrain,]

# fit a model
modFit=train(wage~.,method="gbm",data=training,verbose=FALSE)
print(modFit)

qplot(predict(modFit,testing),wage,data=testing)
```


## model-based predictions (lda, qda, naive bayes)
```{r}
data(iris);library(ggplot2)
table(iris$Species)

inTrain=createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training=iris[inTrain,]
testing=iris[-inTrain,]

# build an lda model on the training set
modlda=train(Species~.,data=training,method="lda")
modnb=train(Species~.,data=training,method="nb")
plda=predict(modlda,testing)
pnb=predict(modnb,testing)
table(plda,pnb)

# disagree on a single value
equalPredictions=(plda==pnb)
qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)
```


# quiz
```{r}
## question 1

library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)

head(segmentationOriginal)
#inTrain=createDataPartition(y=segmentationOriginal$Case,p=0.7,list=FALSE)
training=segmentationOriginal[segmentationOriginal$Case=="Train",]
testing=segmentationOriginal[segmentationOriginal$Case=="Test",]
dim(training); dim(testing)

set.seed(125)

modFit=train(Class~.,method="rpart",data=training)
print(modFit$finalModel)


## question 3
library(pgmm)
data(olive)
olive = olive[,-1]

#inTrain=createDataPartition(y=segmentationOriginal$Case,p=0.7,list=FALSE)
training=olive

set.seed(125)

modFit=train(Area~.,method="rpart",data=training)
print(modFit$finalModel)

newdata = as.data.frame(t(colMeans(olive)))
predict(modFit,newdata=newdata)


### 4

library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]

set.seed(13234)

#age at onset, current alcohol consumption, obesity levels, cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors
modelFit=train(chd~age+alcohol+obesity+tobacco+typea+ldl,
               data=trainSA,method="glm")

missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}

preds=predict(modelFit,trainSA)
missClass(trainSA$chd,preds)

preds=predict(modelFit,testSA)
missClass(testSA$chd,preds)
#confusionMatrix(testing$type,predict(modelFit,testPC))

## 5

library(ElemStatLearn)
data(vowel.train)
data(vowel.test) 

vowel.train$y=as.factor(vowel.train$y);vowel.test$y=as.factor(vowel.test$y)

set.seed(33833)
modFit=train(y~.,data=vowel.train,method="rf",prox=TRUE)
modFit

varImp(modFit)

# use centers of the predicted values
irisP=classCenter(training[,c(3,4)],training$Species,modFit$finalModel$prox) # send it model fit on training data using prox
irisP=as.data.frame(irisP); irisP$Species=rownames(irisP) # that give class centers. so, we've created centers and species data sets.
p=qplot(Petal.Width,Petal.Length,col=Species,data=training) # plot petal length by petal width 
p+geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP) # add points for petal width and length. color based on species. observation centers are x'd. 

# let's look at the two we missed
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="test preds")


```